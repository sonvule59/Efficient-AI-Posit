{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "QxK9u_S3b6Q-",
        "outputId": "b09748b4-ba94-4476-878d-6a42b4c29ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FP32] epoch 1: train CE 0.492  test acc 0.9299  time 8.0s\n",
            "[FP32] epoch 2: train CE 0.206  test acc 0.9514  time 8.7s\n",
            "[FP32] epoch 3: train CE 0.146  test acc 0.9598  time 8.7s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOutput\\n[Posit8] epoch 1: train CE 0.492  test acc 0.9296  time 11.6s\\n[Posit8] epoch 2: train CE 0.206  test acc 0.9515  time 11.4s\\n[Posit8] epoch 3: train CE 0.146  test acc 0.9589  time 11.5s\\n\\n[Posit16] epoch 1: train CE 0.492  test acc 0.9299  time 11.4s\\n[Posit16] epoch 2: train CE 0.206  test acc 0.9516  time 11.8s\\n[Posit16] epoch 3: train CE 0.146  test acc 0.9600  time 11.5s\\n\\nOutput\\n[Posit8] epoch 1: train CE 0.492  test acc 0.9296  time 11.6s\\n[Posit8] epoch 2: train CE 0.206  test acc 0.9515  time 11.4s\\n[Posit8] epoch 3: train CE 0.146  test acc 0.9589  time 11.5s\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "\"\"\"\n",
        "Input: a batch of four random feature vectors (size 6 in the toy run or 784 in the MNIST run).\n",
        "Output: 3-class logits (toy) or 10-class logits (MNIST) **after INT-8 activation quantisation**.\n",
        "Values are multiples of 0.047 (the INT-8 step when `rng = 6`) because activations are uniformly quantised.\n",
        "\n",
        "### Key take-aways\n",
        "\n",
        "1. Posit-16 behaves almost like FP32\n",
        "   Same accuracy, no overflow/denormal fuss, half the memory.\n",
        "2. FP16 and BF16 need a bit more care (learning-rate, scaling) but work.\n",
        "3. **8-bit activations** still converge on an easy task, just lose a few percent accuracy.\n",
        "4. The demo gives you a plug-and-play template: swap one line (`ACTIVE_Q = ...`) to benchmark any other precision scheme you invent.\n",
        "\n",
        "This program trains a tiny two-layer neural network on the MNIST digits, but lets you swap the numeric format used inside the network with one line.\n",
        "It first defines quick “quantisers” for FP32, FP16, BF16, Posit-16, Posit-8, FP8 and INT-8. Whichever quantiser you set in ACTIVE_Q is applied to every weight, bias and activation during the forward pass, while gradients flow normally thanks to a straight-through estimator.\n",
        "The model then runs three training epochs, printing each epoch’s training loss, test accuracy, and time, to show how different precisions affect learning.\n",
        "\"\"\"\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np, struct, math, time, random, os\n",
        "torch.manual_seed(0);  random.seed(0);  np.random.seed(0)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1.  Quantiser zoo\n",
        "#     (same helpers as before, plus wrapper make_quant)\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "def posit_quant_np(x, nbits=16, es=2):\n",
        "    x = np.asarray(x, np.float32)\n",
        "    useed = 2 ** (2 ** es)\n",
        "    max_pos = np.finfo(np.float32).max if nbits > 12 else useed ** ((2 ** (nbits-2))-1)\n",
        "    x = np.clip(x, -max_pos, max_pos)\n",
        "    sign = np.sign(x);  mag = np.abs(x) + 1e-30\n",
        "    log2_mag = np.log2(mag)\n",
        "    scale = 2 ** (nbits - 2)\n",
        "    logq = np.round(log2_mag * scale) / scale\n",
        "    return sign * np.exp2(logq)\n",
        "\n",
        "def fp8_e4m3_np(x):\n",
        "    x = np.asarray(x, np.float32)\n",
        "    sign = np.sign(x);  mag = np.abs(x) + 1e-30\n",
        "    exp  = np.floor(np.log2(mag)).astype(int)\n",
        "    mant = mag / np.exp2(exp) - 1.0\n",
        "    mant_q = np.round(mant * 8) / 8\n",
        "    exp_q  = np.clip(exp, -8, 7)\n",
        "    out = sign * (1.0 + mant_q) * np.exp2(exp_q)\n",
        "    out[exp < -8] = 0.0;  out[exp > 7] = sign[exp > 7]*np.exp2(8)\n",
        "    return out\n",
        "\n",
        "def int_uniform_np(bits, rng=6.0):\n",
        "    step = (2*rng)/(2**bits-1)\n",
        "    return lambda x: np.clip(np.round((x+rng)/step)*step - rng, -rng, rng)\n",
        "\n",
        "def f16_np(x):   return np.array(x, dtype=np.float16).astype(np.float32)\n",
        "def bf16_np(x):  return struct.unpack(\">f\",(struct.pack(\">I\", (np.asarray(x,np.float32).view(np.uint32)>>16)<<16)))[0]\n",
        "\n",
        "class ErrorProbe:\n",
        "    \"\"\"\n",
        "    Hooks into any tensor you pass it and records\n",
        "    1) mean relative error\n",
        "    2) average bits of precision\n",
        "    \"\"\"\n",
        "    def __init__(self, name, baseline_tensor):\n",
        "        self.name = name\n",
        "        self.x_ref = baseline_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def __call__(self, quantised_tensor):\n",
        "        q = quantised_tensor.detach().cpu().numpy()\n",
        "        rel = np.abs(q - self.x_ref) / (np.abs(self.x_ref) + 1e-30)\n",
        "        bits = -np.log2(rel + 1e-30)\n",
        "        self.rel_err = rel.mean()\n",
        "        self.bits    = bits.mean()\n",
        "        return quantised_tensor          # passthrough\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, q_fn):\n",
        "        super().__init__()\n",
        "        self.q = q_fn\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x, probes=None):\n",
        "        # baseline (FP32) copy for comparison\n",
        "        if probes is not None:\n",
        "            probes['inp'] = ErrorProbe('inp', x)\n",
        "\n",
        "        x_q = self.q(x);                   # quantise input\n",
        "        if probes is not None:\n",
        "            probes['inp'](x_q)\n",
        "\n",
        "        h = F.relu(self.q(self.fc1(self.q(x_q))))\n",
        "        if probes is not None:\n",
        "            probes['h'] = ErrorProbe('h', h.detach())   # baseline h\n",
        "            probes['h'](h)\n",
        "\n",
        "        out = self.q(self.fc2(self.q(h)))\n",
        "        if probes is not None:\n",
        "            probes['out'] = ErrorProbe('out', out.detach())\n",
        "            probes['out'](out)\n",
        "        return out\n",
        "\n",
        "class STE(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, t, fn): return torch.from_numpy(fn(t.cpu().numpy())).to(t.device)\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):    return g, None\n",
        "def make_q(fn): return lambda t: STE.apply(t, fn)\n",
        "\n",
        "QUANTISERS = {\n",
        "    \"FP32\" : lambda t: t,                         # identity\n",
        "    \"Posit16\": make_q(lambda x: posit_quant_np(x,16,2)),\n",
        "    \"FP16\"  : make_q(f16_np),\n",
        "    \"BF16\"  : make_q(bf16_np),\n",
        "    \"INT8\"  : make_q(int_uniform_np(8)),\n",
        "    \"Posit8\" : make_q(lambda x: posit_quant_np(x,8,1)),\n",
        "    \"FP8\"    : make_q(fp8_e4m3_np),\n",
        "}\n",
        "\n",
        "ACTIVE_Q = \"FP32\"          # <<< change to FP32 / FP16 / BF16 / INT8\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 2.  Dataset (MNIST 28×28 → flatten 784)\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "tr = transforms.Compose([transforms.ToTensor(),\n",
        "                         transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_ds = datasets.MNIST(root=\".\", train=True,  transform=tr, download=True)\n",
        "test_ds  = datasets.MNIST(root=\".\", train=False, transform=tr)\n",
        "\n",
        "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "test_ld  = torch.utils.data.DataLoader(test_ds, batch_size=1024)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 3.  Quantised MLP\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "class QLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, q): super().__init__(); self.w=nn.Parameter(torch.randn(out_f,in_f)*0.02); self.b=nn.Parameter(torch.zeros(out_f)); self.q=q\n",
        "    def forward(self,x): return self.q(F.linear(self.q(x), self.q(self.w), self.q(self.b)))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,q): super().__init__(); self.fc1=QLinear(784,256,q); self.fc2=QLinear(256,10,q); self.q=q\n",
        "    def forward(self,x): return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "net = Net(QUANTISERS[ACTIVE_Q]).to(\"cpu\")\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "ce  = nn.CrossEntropyLoss()\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 4.  Training 3 epochs\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "for epoch in range(3):\n",
        "    net.train();  t0=time.time();  loss_cum=0\n",
        "    for xb,yb in train_ld:\n",
        "        opt.zero_grad()\n",
        "        loss=ce(net(xb), yb); loss.backward(); opt.step()\n",
        "        loss_cum+=loss.item()*xb.size(0)\n",
        "\n",
        "# probes = {}\n",
        "# # _ = net(batch, probes=probes)\n",
        "# # for name, p in probes.items():\n",
        "# #     print(f\"{name}: rel-err {p.rel_err:.2e} | bits {p.bits:.2f}\")\n",
        "# for epoch in range(3):\n",
        "#     net.train(); loss_cum = 0\n",
        "#     for i, (xb, yb) in enumerate(train_ld, 1):\n",
        "#         ...\n",
        "#         if i % 50 == 0:   # every 50 mini-batches\n",
        "#             print(f\"epoch {epoch+1} batch {i} / {len(train_ld)}\")\n",
        "\n",
        "    # test\n",
        "    net.eval(); acc=0\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in test_ld:\n",
        "            pred=net(xb).argmax(1);  acc+=(pred==yb).sum().item()\n",
        "    print(f\"[{ACTIVE_Q}] epoch {epoch+1}: \"\n",
        "          f\"train CE {loss_cum/len(train_ds):.3f}  \"\n",
        "          f\"test acc {acc/len(test_ds):.4f}  \"\n",
        "          f\"time {time.time()-t0:.1f}s\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Output\n",
        "[Posit8] epoch 1: train CE 0.492  test acc 0.9296  time 11.6s\n",
        "[Posit8] epoch 2: train CE 0.206  test acc 0.9515  time 11.4s\n",
        "[Posit8] epoch 3: train CE 0.146  test acc 0.9589  time 11.5s\n",
        "\n",
        "[Posit16] epoch 1: train CE 0.492  test acc 0.9299  time 11.4s\n",
        "[Posit16] epoch 2: train CE 0.206  test acc 0.9516  time 11.8s\n",
        "[Posit16] epoch 3: train CE 0.146  test acc 0.9600  time 11.5s\n",
        "\\nOutput\\n[Posit8] epoch 1: train CE 0.492  test acc 0.9296  time 11.6s\\n[Posit8] epoch 2: train CE 0.206  test acc 0.9515  time 11.4s\\n[Posit8] epoch 3: train CE 0.146  test acc 0.9589  time 11.5s\\n\\n\\n\n",
        "\"\"\"\n"
      ]
    }
  ]
}